{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2006 = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"nullValue\",\"NA\") \\\n",
    ".option(\"inferSchema\", \"true\").load(\"2006.csv\")\n",
    "df2007 = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"nullValue\",\"NA\") \\\n",
    ".option(\"inferSchema\", \"true\").load(\"2007.csv\")\n",
    "df2008 = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"nullValue\",\"NA\") \\\n",
    ".option(\"inferSchema\", \"true\").load(\"2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Partitions\")\n",
    "df2006.rdd.getNumPartitions()\n",
    "df2007.rdd.getNumPartitions()\n",
    "df2008.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Elements\")\n",
    "df2006.count()\n",
    "df2007.count()\n",
    "df2008.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fusionar\")\n",
    "df1 = df2006.union(df2007).union(df2008)\n",
    "df1.count()\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.select(\"Year\", \"Month\", \"Origin\", \"Dest\", \"ArrDelay\", \"DepDelay\")\n",
    "df3 = df2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"retard total\")\n",
    "from pyspark.sql.functions import expr\n",
    "df4 = df3.withColumn(\"SumDelay\", expr(\"ArrDelay + DepDelay\"))\n",
    "df4.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quin és el retard mig, pels anys 2006, 2007 i 2008, per cada aeroport d’origen? Això és fa amb\n",
    "la funció groupBy, que ens permet agrupar les dades segons el valor d’una determinada columna.\n",
    "Hem de fer servir la funció agg per indicar quina operació volem fer sobre els valors de les columnes\n",
    "agrupades.\n",
    "Amb la següent instrucció es crea un DataFrame amb dues columnes: la primera columna és\n",
    "l’aeroport d’origen i la segona columna contindrà el valor mig del retard total.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import avg\n",
    "df5 = df4.groupBy(\"Origin\").agg(avg(\"SumDelay\"))\n",
    "df5.show()\n",
    "df5.count()\n",
    "df6 = df5.withColumnRenamed(\"avg(SumDelay)\", \"Average Delay\")\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercici\n",
    "#Ordeneu els resultats per retard mig obtingut, de menor a major, i mostreu els 20 primers resultats\n",
    "#per pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercici\n",
    "\"\"\"\n",
    "Es proposa calcular el valor mig del retard només per a la ciutat de Nova York, agrupat per mes.\n",
    "Hi ha un mes en que el retard és més gran que l’altre? Proveu-ho! Quina conclusió traieu? Quins\n",
    "són els mesos en què hi ha més retard?\n",
    "Podem agrupar per múltiples criteris. En aquest cas agrupem per any i origen\n",
    "df5 = df4.groupBy(\"Year\", \"Origin\").agg(avg(\"SumDelay\"))\n",
    "df5.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercici\n",
    "\"\"\"\n",
    "Es proposa calcular el valor mig del retard només per a la ciutat de Nova York, agrupat per mes.\n",
    "Hi ha un mes en que el retard és més gran que l’altre? Proveu-ho! Quina conclusió traieu? Quins\n",
    "són els mesos en què hi ha més retard?\n",
    "Anem a comptar el nombre de vols que hi ha entre els aeroports. Spark ens ofereix les funcions\n",
    "necessàries per fer-ho:\n",
    ">‌>‌> from pyspark.sql.functions import lit, count\n",
    ">‌>‌> df5 = df4.groupBy(\"Origin\", \"Dest\").agg(count(lit(1)))\n",
    "La funció count(lit(1)) permet fer una suma del nombre d’elements en comptes de fer-ho sobre\n",
    "els valors d’una determinada columna del DataFrame.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercici\n",
    "#Quin són els vols més habituals? I entre quins aeroports es realitzen? Nova York? Boston? Los\n",
    "#Angeles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User functions\n",
    "def retard(temps):\n",
    "    if (temps > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "spark.udf.register(\"retard_spark\", retard, IntegerType())\n",
    "df5 = df4.withColumn(\"Retard\", expr(\"retard_spark(SumDelay)\"))\n",
    "df5.show()\n",
    "Ara podem agrupar i veure el percentatge de retard dels avions\n",
    "df5.groupBy(\"Origin\").agg(avg(\"Retard\")).sort(\"avg(Retard)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercici\n",
    "\"\"\"Es proposa calcular ara el percentatge mig dels vols amb retard per a la ciutat de Nova York, agrupat\n",
    "per mes. Hi ha un mes en que el retard és més gran que l’altre? Proveu-ho! Quina conclusió traieu?\n",
    "Els resultats són congruents amb el que heu obtingut abans?\n",
    "En aquest exemple hem vist com podem aplicar funcions d’usuari pròpies que s’apliquen als\n",
    "registres individuals del DataFrame. Spark també ofereix la possibilitat de definir funcions d’usuari\n",
    "a aplicar a l’hora d’agrupar dades. Això ens permet aplicar altres funcions de les que ja estan\n",
    "definides al Spark (max, min, avg, ...). Podreu trobar la forma de fer-ho a la documentació de Spark\n",
    "(tècnicament s’anomenen User Defined Aggregation Functions).\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
