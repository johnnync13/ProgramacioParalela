{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip3 install pyspark\n",
    "!pip install findspark\n",
    "!pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"nullValue\",\"NA\") \\\n",
    ".option(\"inferSchema\", \"true\").load(\"2007.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Partitions:\", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+--------+\n",
      "|Origin|Dest|ArrDelay|DepDelay|\n",
      "+------+----+--------+--------+\n",
      "|   SMF| ONT|       1|       7|\n",
      "|   SMF| PDX|       8|      13|\n",
      "|   SMF| PDX|      34|      36|\n",
      "|   SMF| PDX|      26|      30|\n",
      "|   SMF| PDX|      -3|       1|\n",
      "|   SMF| PDX|       3|      10|\n",
      "|   SMF| PHX|      47|      56|\n",
      "|   SMF| PHX|      -2|       9|\n",
      "|   SMF| PHX|      44|      47|\n",
      "|   SMF| PHX|      -7|       3|\n",
      "|   SMF| PHX|     -11|       1|\n",
      "|   SMF| PHX|      52|      52|\n",
      "|   SMF| SAN|      45|      53|\n",
      "|   SMF| SAN|     -17|      -5|\n",
      "|   SMF| SAN|      -5|       6|\n",
      "|   SMF| SAN|      33|      44|\n",
      "|   SMF| SAN|      -9|       0|\n",
      "|   SMF| SAN|      -7|       2|\n",
      "|   SMF| SAN|     -11|       1|\n",
      "|   SMF| SAN|      36|      29|\n",
      "+------+----+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Eliminar columnes\n",
    "df2 = df.drop(\"Flightfrom pyspark.sql.functions import exprNum\",\"TailNum\",\"UniqueCarrier\")\n",
    "#New frame with columns\n",
    "df2 = df.select(\"Origin\", \"Dest\", \"ArrDelay\", \"DepDelay\")\n",
    "df2.show()\n",
    "#Eliminar files on hi ha NA\n",
    "df3 = df2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|DepDelay|ArrDelay|SumDelay|\n",
      "+--------+--------+--------+\n",
      "|       7|       1|       8|\n",
      "|      13|       8|      21|\n",
      "|      36|      34|      70|\n",
      "|      30|      26|      56|\n",
      "|       1|      -3|      -2|\n",
      "|      10|       3|      13|\n",
      "|      56|      47|     103|\n",
      "|       9|      -2|       7|\n",
      "|      47|      44|      91|\n",
      "|       3|      -7|      -4|\n",
      "+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "##Suma, expr we can do expressions\n",
    "df4 = df3.withColumn(\"SumDelay\", expr(\"ArrDelay + DepDelay\"))\n",
    "#Action, no transformation\n",
    "df4.select(\"DepDelay\", \"ArrDelay\", \"SumDelay\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|max(SumDelay)|min(SumDelay)|\n",
      "+-------------+-------------+\n",
      "|         5199|         -617|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "#es recorre tot el df\n",
    "df4.select(max(\"SumDelay\"),min(\"SumDelay\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------------+\n",
      "|max(SumDelay)|min(SumDelay)|    avg(SumDelay)|\n",
      "+-------------+-------------+-----------------+\n",
      "|         5199|         -617|21.55425998256014|\n",
      "+-------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df4.select(max(\"SumDelay\"),min(\"SumDelay\"),avg(\"SumDelay\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Origin: string, Dest: string, ArrDelay: int, DepDelay: int, SumDelay: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temporal save in cache or ram\n",
    "df4.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+--------+--------+\n",
      "|Origin|Dest|ArrDelay|DepDelay|SumDelay|\n",
      "+------+----+--------+--------+--------+\n",
      "|   SMF| PDX|      -3|       1|      -2|\n",
      "|   SMF| PHX|      -7|       3|      -4|\n",
      "|   SMF| PHX|     -11|       1|     -10|\n",
      "|   SMF| SAN|     -17|      -5|     -22|\n",
      "|   SMF| SAN|      -9|       0|      -9|\n",
      "|   SMF| SAN|      -7|       2|      -5|\n",
      "|   SMF| SAN|     -11|       1|     -10|\n",
      "|   SMF| SAN|      -6|       3|      -3|\n",
      "|   SMF| SAN|     -14|       0|     -14|\n",
      "|   SMF| SAN|      -9|      -5|     -14|\n",
      "|   SMF| SNA|      -4|       0|      -4|\n",
      "|   SMF| SNA|      -8|       2|      -6|\n",
      "|   SMF| SNA|     -16|      -4|     -20|\n",
      "|   SMF| SNA|      -7|       0|      -7|\n",
      "|   SMF| SNA|     -15|      -4|     -19|\n",
      "|   SNA| MDW|     -18|       0|     -18|\n",
      "|   SNA| OAK|      -1|       0|      -1|\n",
      "|   SNA| OAK|       0|      -1|      -1|\n",
      "|   SNA| OAK|      -2|       0|      -2|\n",
      "|   SNA| OAK|      -1|       0|      -1|\n",
      "+------+----+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3676937"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = df4.where(\"SumDelay < 0\")\n",
    "df5.show() #accio, nomes mostra els primer elements\n",
    "df3.count()\n",
    "df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+--------+--------+\n",
      "|Origin|Dest|ArrDelay|DepDelay|SumDelay|\n",
      "+------+----+--------+--------+--------+\n",
      "|   JFK| CLE|     -13|     -19|     -32|\n",
      "|   JFK| CLE|     -25|      -3|     -28|\n",
      "|   JFK| CLE|     -25|      -8|     -33|\n",
      "|   JFK| CLE|       4|     -14|     -10|\n",
      "|   JFK| CLE|     -17|      -2|     -19|\n",
      "|   JFK| CLE|      -4|      -3|      -7|\n",
      "|   JFK| CLE|      -4|      -3|      -7|\n",
      "|   JFK| CLE|      -1|      -8|      -9|\n",
      "|   JFK| CLE|     -17|     -10|     -27|\n",
      "|   JFK| CLE|     -30|      -9|     -39|\n",
      "|   JFK| CLE|       3|      -5|      -2|\n",
      "|   JFK| CLE|      -6|     -10|     -16|\n",
      "|   JFK| CLE|     -20|      -4|     -24|\n",
      "|   JFK| CLE|     -12|      -6|     -18|\n",
      "|   JFK| CLE|       5|      -8|      -3|\n",
      "|   JFK| IAD|     -15|      -3|     -18|\n",
      "|   JFK| IAD|      -4|       1|      -3|\n",
      "|   JFK| IAD|     -13|     -16|     -29|\n",
      "|   JFK| IAD|      -7|       6|      -1|\n",
      "|   JFK| CLT|      -8|       0|      -8|\n",
      "+------+----+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53321"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = df4.where(\"SumDelay < 0\").where(\"Origin == 'JFK'\")\n",
    "df5.show()\n",
    "df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53321"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L’altre funció de filtratge disponible es diu filter. És molt similar a la funció where, amb l’a-\n",
    "#vantatge que es poden fer servir varibles de Python per establir els criteris sobre les condicions de\n",
    "#filtratge.\n",
    "from pyspark.sql.functions import col\n",
    "i = 0\n",
    "city = \"JFK\"\n",
    "df5 = df4.filter(col(\"SumDelay\") < i).filter(col(\"Origin\") == city)\n",
    "df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------------+\n",
      "|max(SumDelay)|min(SumDelay)|    avg(SumDelay)|\n",
      "+-------------+-------------+-----------------+\n",
      "|         3111|          -90|35.16175687302823|\n",
      "+-------------+-------------+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|sum(SumDelay)|\n",
      "+-------------+\n",
      "|      4290965|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercici\n",
    "\"\"\"\n",
    "Atesos els exemples que hem vist, ens podem preguntar: donat l’aeroport d’origen de Nova York,\n",
    "quin és el retard total (columna SumDelay) mínim, màxim i mig associat? Podeu provar també la\n",
    "funció sum, una transformació que realitzarà la suma de la columna que especifiqueu.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import sum\n",
    "\"Dos soluciones diferentes\"\n",
    "df5 = df4.where(\"Origin == 'JFK'\").select(max(\"SumDelay\"),min(\"SumDelay\"),avg(\"SumDelay\")).show()\n",
    "df5 = df4.where(\"Origin == 'JFK'\").select(\"SumDelay\").groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+--------+--------+\n",
      "|Origin|Dest|ArrDelay|DepDelay|SumDelay|\n",
      "+------+----+--------+--------+--------+\n",
      "|   AKN| ANC|    -312|    -305|    -617|\n",
      "|   CHA| ATL|    -175|    -165|    -340|\n",
      "|   ANC| FAI|    -162|    -165|    -327|\n",
      "|   AUS| ATL|    -132|    -124|    -256|\n",
      "|   ATL| AVL|    -116|    -111|    -227|\n",
      "|   ANC| SEA|     -13|    -168|    -181|\n",
      "|   SAN| OKC|    -157|     -19|    -176|\n",
      "|   HNL| KOA|     -89|     -82|    -171|\n",
      "|   SFO| HNL|       8|    -169|    -161|\n",
      "|   ADK| ANC|     -83|     -72|    -155|\n",
      "|   SJC| SBA|     -82|     -67|    -149|\n",
      "|   ITO| HNL|     -74|     -71|    -145|\n",
      "|   ADK| ANC|     -78|     -67|    -145|\n",
      "|   GNV| ATL|     -82|     -62|    -144|\n",
      "|   ANC| SEA|      -5|    -137|    -142|\n",
      "|   PHL| SJU|     -79|     -60|    -139|\n",
      "|   ADK| ANC|     -79|     -58|    -137|\n",
      "|   YAK| JNU|     -73|     -64|    -137|\n",
      "|   KOA| HNL|     -72|     -65|    -137|\n",
      "|   DHN| ATL|     -75|     -60|    -135|\n",
      "+------+----+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----+--------+--------+--------+\n",
      "|Origin|Dest|ArrDelay|DepDelay|SumDelay|\n",
      "+------+----+--------+--------+--------+\n",
      "|   PBI| DTW|    2598|    2601|    5199|\n",
      "|   ALO| MSP|    1942|    1956|    3898|\n",
      "|   HNL| MSP|    1848|    1831|    3679|\n",
      "|   FWA| DTW|    1715|    1736|    3451|\n",
      "|   FAI| MSP|    1665|    1689|    3354|\n",
      "+------+----+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "df5 = df4.sort(asc(\"SumDelay\"))\n",
    "df5.show()\n",
    "df5 = df4.sort(desc(\"SumDelay\")).limit(5)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UNIQUE Elements\n",
    "df5 = df4.select(\"Origin\").distinct()\n",
    "df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Dest|\n",
      "+----+\n",
      "| BGM|\n",
      "| PSE|\n",
      "| DLG|\n",
      "| INL|\n",
      "| MSY|\n",
      "| GEG|\n",
      "| SNA|\n",
      "| BUR|\n",
      "| GRB|\n",
      "| GTF|\n",
      "| IDA|\n",
      "| GRR|\n",
      "| LWB|\n",
      "| EUG|\n",
      "| PSG|\n",
      "| PVD|\n",
      "| GSO|\n",
      "| MYR|\n",
      "| ISO|\n",
      "| OAK|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercici\n",
    "#Quantes destinacions diferents hi ha? A més, què és el que fa la següent operació?\n",
    "df4.select(\"Origin\",\"Dest\").distinct().count()\n",
    "df4.select(\"Dest\").distinct().show()\n",
    "df4.select(\"Dest\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dades = df4.limit(5).collect()\n",
    "dades\n",
    "dades[0]\n",
    "dades[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0c45edb65cc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf4_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf4_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf4_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df4_one.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "#Escriure a disc\n",
    "df4.rdd.getNumPartitions()\n",
    "#Per defecte, si ara escrivíssim les dades a disc, es crearien tants fitxers com particions hi ha\n",
    "df4_one = df4.coalesce(1)\n",
    "df4_one.rdd.getNumPartitions()\n",
    "df4_one = spark.write.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".save(\"df4_one.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Exercici\n",
    "\"\"\"És interessant que feu un experiment: abans hem ordenat els elements de df4 pel valor de SumDelay.\n",
    "Intenteu ordenar ara els valors de df4_one. Hi haurà alguna diferència en l’eficiència de l’ordenació?\n",
    "Feu servir alguna eina gràfica que us mostri el consum de CPU per veure-ho. Per a què és útil tenir\n",
    "les dades dividides en particions?\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
