{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDALab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bx2JSyCHfCq",
        "colab_type": "text"
      },
      "source": [
        "# Programaciò paral·lela\n",
        "## Pràctica 2: Color conversion in CUDA\n",
        "\n",
        "Following the previous laboratory based on OpenMP, now we are going to work on the same simple algorithm, in CUDA.\n",
        "\n",
        "The goal of this lab is to learn the basics of CUDA kernels, and CUDA Host code.\n",
        "\n",
        "This new ipython notebook format, will make things easyer since explanation and code will coexist in the same document, and it will be very clear what you need to do.\n",
        "\n",
        "Additionally, having the Colab platform available with NVIDIA GPU's makes it simpler than ever. You can learn CUDA from any system, Mac, Windows, Linux, and any hardware, Intel, AMD, NVIDIA, and possibliy even ARM on tablets. You only need a web browser compatible with Colab.\n",
        "\n",
        "## Structure of the Lab (Pràctica)\n",
        "You already know the algorithm from the previous lab, but you may not be familiar with this environment.\n",
        "\n",
        "First we will try to understand a bit this environment, and then we will explain section by section what you have to do. There are 6 sections.\n",
        "\n",
        "You will have to complete code in the 6 sections, and perform experiments and comment the results in a separated report. Use tables and figures that support both the results you collected and the arguments you make to justify the results.\n",
        "\n",
        "## The collab environment for CUDA\n",
        "\n",
        "First of all, you should know that we are executing an iPython notebook in a Google Colab session. The notebook is preconfigured with the type of execution environment we need, a GPU execution environment. But the files we generate, and the pluggins we install or enable, reside on the Google Colab session. All this will be removed when we exit the session either manually or implicitly by closing the broser.\n",
        "\n",
        "In order to have a GPU available when creating a new notebook, you only have to select the execution environment.\n",
        "In Spanish, go to \"Entorno de ejecución->Cambiar tipo de entorno de ejecución\" and then select GPU.\n",
        "\n",
        "But as we already mentioned, this notebook is already configured, so you don't need to do it again.\n",
        "\n",
        "Now, the first thing we will see is that we have the nvcc compiler. We can call many bash commands with ! as the first character, in a code block. Next you will find a code block with a call to nvcc (the nvidia CUDA compiler) with a flag that asks for the compiler version.\n",
        "\n",
        "Click on the block and then a play button will appear on the left. Click on the play button. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNXYi-1xD-Zs",
        "colab_type": "code",
        "outputId": "9b8546c2-9ab3-4a47-8917-40a2f1db52e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!rm dades.txt # Netejem el fitxer on guardarem les dades\n",
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'dades.txt': No such file or directory\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4rhjdqSImo2",
        "colab_type": "text"
      },
      "source": [
        "You can also execute it by placing the cursor inside the code block and pressing Shift+Enter\n",
        "\n",
        "Next you need to install a pluggin, that does not come with the notebook. In the following code block you have the code line to be executed. You will have to execute this code every time you open the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l1NOZW5ET_p",
        "colab_type": "code",
        "outputId": "9aad4d2e-fa1e-4640-8a52-0b00ab2d24c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-pvks5bjf\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-pvks5bjf\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=87d4ade2297439d0006c9972cbc47dbf7abf293cd3e3be1514ff41001233502e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cbvw0nr9/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuMz7OOYJVfk",
        "colab_type": "text"
      },
      "source": [
        "Now, you can compile and execute CUDA code, just by puting the same code you would put ina .cu file, just by adding %%cu as the first line.\n",
        "\n",
        "Next, you have a code example. Try it! Read the comments to help you understand it. It will be very useful for the tasks you have to do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x03T7kcmEgu4",
        "colab_type": "code",
        "outputId": "e5aa578e-f0bb-49c3-fc0f-f1cbedf68c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Function to print the cuda errors\n",
        "void cuCheck(cudaError_t err) {\n",
        "    if(err!=cudaSuccess) {\n",
        "          printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "}\n",
        "\n",
        "// This macros help with capturing the possible cuda errors and printing\n",
        "// the error name to help the developer.\n",
        "// Kernels are always asynchronous with respect to the Host, so they don't return\n",
        "// any value. Then to see if any error happened, you should call cudaGetLastError\n",
        "// and pass the result to cuCheck()\n",
        "// Use the macros instead, to make it simpler.\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())\n",
        "\n",
        "// Device code or Kernel\n",
        "__global__ void add(int a, int b, int* __restrict d_c) {\n",
        "    *d_c = a * b;\n",
        "}\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    \n",
        "    // Host variables a & b\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "\n",
        "    // Host variable that will store a Device pointer wich we can later on \n",
        "    // download to the Host.\n",
        "    // As this variable will contain pointers that are only valid in\n",
        "    // the Device (the GPU) it will be invalid to access them from\n",
        "    // Host code. We only can use them in the right cuda API calls\n",
        "    // or inside a cuda Kernel.\n",
        "    // So in this part of the code you won't be able to do d_c[0], for instance\n",
        "    int *d_c;\n",
        "\n",
        "    // Size of the data contained in variables a, b and c.\n",
        "    int dataSize = sizeof(int);\n",
        "\n",
        "    // Reserve Device memory using the cuda API\n",
        "    // cudaMalloc will place a Device pointer inside d_c.\n",
        "    CU_CHECK(\n",
        "        cudaMalloc((void **)&d_c, dataSize)\n",
        "    );\n",
        "\n",
        "    // Launch add() kernel on GPU\n",
        "    // Notice that a and b are not pointers. Therefore the kernel call will\n",
        "    // copy their values but the variables inside the kernel will not be the same.\n",
        "    // If we modify a and b inside the kernel, it will not change a and b in this\n",
        "    // Host code. This, indeed is the same behavior as any C/C++ function call.\n",
        "    // In the case of d_c, it will copy the pointer contained in d_c, \n",
        "    // so we will be able to modify the contents of d_c from the kernel. But to read \n",
        "    // them from this Host code, we will have to do something else.\n",
        "    add<<<1,1>>>(a, b, d_c);\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "\n",
        "    CU_CHECK(\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost)\n",
        "    );\n",
        "\n",
        "    int numDevs=0;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceCount(&numDevs)\n",
        "    );\n",
        "\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceProperties(&prop, 0)\n",
        "    );\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\",a,b,h_c);\n",
        "    // Cleanup\n",
        "    CU_CHECK(\n",
        "      cudaFree(d_c)\n",
        "    );\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla P4\n",
            "  Memory Clock Rate (KHz): 3003000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 192.192000\n",
            "\n",
            "Num devices 1\n",
            "Result of multiplying 3 * 5 is 15\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajyZJvBCZR7p",
        "colab_type": "text"
      },
      "source": [
        "Ok, cool! But what if I want to have some code in a .h file, the cuda kernels in an other .h file, and include both so that I can reuse code?\n",
        "\n",
        "Ok, let's try to put the macros and cuCheck function in a .h file, the kernel in an other .h file and the rest in a .cu file, and compile and execute everything. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlObGBaEZ5M7",
        "colab_type": "code",
        "outputId": "7e019d2b-b467-44e6-84b0-55ff13201faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name utils.h\n",
        "#include <iostream>\n",
        "\n",
        "void cuCheck(cudaError_t err, const std::string message = \"CUDA error:\") {\n",
        "  if(err!=cudaSuccess) {\n",
        "    std::cout << message << \" ERROR \" << cudaGetErrorString(err) << std::endl;\n",
        "  }\n",
        "}\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK2(a, b) cuCheck(a, b)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/utils.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEoYz9uug7om",
        "colab_type": "code",
        "outputId": "b547d370-48d7-4a8a-ac77-7a3baae750d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name kernels.h\n",
        "__global__ void add(int a, int b, int* __restrict d_c) {\n",
        "    *d_c = a * b;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/kernels.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdHjgZRHlSGe",
        "colab_type": "code",
        "outputId": "4350f302-4d5a-4cc5-d9ef-71615f96d70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/kernels.h\"\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "    int *d_c;\n",
        "    int dataSize = sizeof(int);\n",
        "    CU_CHECK(cudaMalloc((void **)&d_c, dataSize));\n",
        "    add<<<1,1>>>(a, b, d_c);\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "    CU_CHECK(cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost));\n",
        "    int numDevs=0;\n",
        "    CU_CHECK(cudaGetDeviceCount(&numDevs));\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\",a,b,h_c);\n",
        "    CU_CHECK(cudaFree(d_c));\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla P4\n",
            "  Memory Clock Rate (KHz): 3003000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 192.192000\n",
            "\n",
            "Num devices 1\n",
            "Result of multiplying 3 * 5 is 15\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8rWgfJ8hN3m",
        "colab_type": "text"
      },
      "source": [
        "VERY IMPORTANT!!! On each Colab session, the GPU that Google Colab provides can be different. Take it into account when you perform experiments, so that you compare results for the same GPU.\n",
        "\n",
        "If you have to repeat all the experiments, well, it's not that hard, just click play in all the code blocks one by one.\n",
        "\n",
        "Great!! Now we can start the lab :-D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX0Yv50Wh4nY",
        "colab_type": "text"
      },
      "source": [
        "##Section 1:\n",
        "\n",
        "Try to complete the following code, and make it compile. Remember that you have some slides and documents, and the CUDA API specification in the following link: https://docs.nvidia.com/cuda/cuda-runtime-api/index.html\n",
        "\n",
        "Also, you can search in Google, things like \"How to allocate CUDA memory\". And so on. Be brave! Is not so difficult.\n",
        "\n",
        "### First, complete the allocation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Bj54YdVsjI",
        "colab_type": "code",
        "outputId": "09f4a5de-9016-42e3-d965-770e6b36437b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cudaSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaMalloc(d_rgba, sizeof(uchar4)*width*height), \"Alloc d_rgba:\");\n",
        "}\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpy(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemset(d_rgba, 0, width*height*sizeof(uchar4)), \"Memset d_rgba:\");\n",
        "}\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaFree(d_brg), \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaFree(d_rgba), \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/memory_functions.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s30mTBEIWcX6",
        "colab_type": "text"
      },
      "source": [
        "### When completed, test that they work with this small main function. If you execute it without completing the previous code, it will show some errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tup9fxlwWl64",
        "colab_type": "code",
        "outputId": "fd26265a-c029-478f-efa8-fd024ceb50a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cu\n",
        "#include <cuda.h>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "\n",
        "#define WIDTH 10\n",
        "#define HEIGHT 10\n",
        "\n",
        "int main() {\n",
        "\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfdiwiHZKPT",
        "colab_type": "text"
      },
      "source": [
        "### Ok, now that we have the allocation, copy and free functions implemented, let's continue with the CPU function that will check the results. This one it's already implemented, you only need to click play to have it available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fywcbihaBaR",
        "colab_type": "code",
        "outputId": "f226677e-0641-4ab8-b3cf-b3ba1610abbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name check_results.h\n",
        "bool checkResults(uchar4* rgba, uchar3* brg, int size) {\n",
        "  bool correct = true;\n",
        "  for (int i=0; i < size; ++i) {\n",
        "    // In case you want to see actual values\n",
        "    if (i==0 || i==1 || i==(size-1) ) {\n",
        "      unsigned char x, y, z, w;\n",
        "      x = rgba[i].x;\n",
        "      y = rgba[i].y;\n",
        "      z = rgba[i].z;\n",
        "      w = rgba[i].w;\n",
        "      std::cout << \"First position x=\" << (unsigned int)x << \" y=\" << (unsigned int)y << \" z=\" << (unsigned int)z << \" w=\" << (unsigned int)w << std::endl;\n",
        "    }\n",
        "    correct &= rgba[i].x == brg[i].y;\n",
        "    correct &= rgba[i].y == brg[i].z;\n",
        "    correct &= rgba[i].z == brg[i].x;\n",
        "    correct &= rgba[i].w == 255;\n",
        "    /*if(!correct)\n",
        "    {\n",
        "        std::cout << \"First position x=\" << (unsigned int)rgba[i].x << \" y=\" << (unsigned int)rgba[i].y << \" z=\" << (unsigned int)rgba[i].z << \" w=\" << (unsigned int)rgba[i].w << std::endl;\n",
        "        correct = true;\n",
        "    }*/\n",
        "  }\n",
        "  return correct;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/check_results.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl-YagCWaZOS",
        "colab_type": "text"
      },
      "source": [
        "### Now the interesting part, the kernel and the code to configure and launch it. The kernel it's almost exactly the same code as the OpenMP lab, only we replaced the forloops with something that you need to implement.\n",
        "\n",
        "Remember, that we have threads with indexes. This indexes are used to tell each CUDA thread, which data do they have to read or write.\n",
        "\n",
        "The structs that contain those indexes are in the documentation you have available in campusvirtual. Please check the docs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3Bww3PbMgw",
        "colab_type": "code",
        "outputId": "aa8f9111-ab44-435a-fe29-e1e874a5296d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include <stdio.h>\n",
        "\n",
        "\n",
        "// BIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = threadIdx.x + (blockIdx.x * blockDim.x); //Use the thread id and block id's to compute x \n",
        "  int y = threadIdx.y + (blockIdx.y * blockDim.y); //Use the thread id and block id's to compute y\n",
        "\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width && y < height) {\t\n",
        "\t    rgba[width * y + x].x = brg[width * y + x].y;\n",
        "\t    rgba[width * y + x].y = brg[width * y + x].z;\n",
        "\t    rgba[width * y + x].z = brg[width * y + x].x;\n",
        "\t    rgba[width * y + x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(256, 4, 1);\n",
        "  dim3 grid(ceil(width/(float)block.x),ceil(height/(float)block.y) , 1);\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_brg, d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  FILE* fp = fopen(\"dades.txt\", \"a\");\n",
        "  fprintf(fp, \"%lf\\n\", (double) duration);\n",
        "  fclose(fp);\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2Ty-GmcSY_",
        "colab_type": "text"
      },
      "source": [
        "### MAIN EXPERIMENT \n",
        "Try all the previous code, with the following main. If you did not finish all the previous code, this file will show some execution errors.\n",
        "\n",
        "The code is divided in two parts, one to define the parameters of the experiment and the other one is the main function with the experiment it self.\n",
        "\n",
        "The experiment is the code that creates a BRG image in CPU, allocates GPU memory, copies the BRG image to GPU memory, and executes a GPU kernel to convert the BRG image into a RGBA image. The output of the kernel is another GPU pointer, so after the kernel execution, we have to copy back the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwcU52z8B--N",
        "colab_type": "code",
        "outputId": "88fefc63-037f-4dac-b7b3-27762eb7ef01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name experiment_settings.h\n",
        "#pragma once\n",
        "#define WIDTH 3840\n",
        "#define HEIGHT 2160\n",
        "#define EXPERIMENT_ITERATIONS 100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/experiment_settings.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT9ep3h_ijOO",
        "colab_type": "code",
        "outputId": "9f18982b-f0ae-4472-df5a-3b93b6b5ad25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = HEIGHT/3;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  \n",
        "  // Prepare and copy data to GPU\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS);\n",
        "\n",
        "  // Copy data back from GPU to CPU, without streams\n",
        "  CU_CHECK2(cudaMemcpy(h_rgba, d_rgba, sizeof(uchar4)*WIDTH*HEIGHT, cudaMemcpyDeviceToHost), \"Cuda memcpy Device to Host: \");\n",
        "    \n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/experiment.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5RA9zDI7Z0V",
        "colab_type": "code",
        "outputId": "58ac3d47-2116-4976-f00b-ff3a3cc8256b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 52725us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=255 z=0 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GqpbCcVdK83",
        "colab_type": "text"
      },
      "source": [
        "##Section 2:\n",
        "Implement a version of the kernel and launcher that uses a one dimensional cuda GRID. That is, there is no more x and y, only x.\n",
        "\n",
        "Modify the code below, click play, and then click play in the Main Experiment block, in Section 1.\n",
        "\n",
        "Try different values of BLOCK_SIZE.\n",
        "\n",
        "Check if there is any execution time improvement, compared to Section 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLPY7trNd1SG",
        "colab_type": "code",
        "outputId": "8286f6cd-b74b-4842-982e-7466beb1ffcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = threadIdx.x + (blockIdx.x * blockDim.x); //Use the thread id and block id's to compute x \n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\t\n",
        "\t    rgba[x].x = brg[x].y;\n",
        "\t    rgba[x].y = brg[x].z;\n",
        "\t    rgba[x].z = brg[x].x;\n",
        "\t    rgba[x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_brg, d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  FILE* fp = fopen(\"dades.txt\", \"a\");\n",
        "  fprintf(fp, \"%lf\\n\", (double) duration);\n",
        "  fclose(fp);\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozsLq2rt__xl",
        "colab_type": "code",
        "outputId": "a9a25f85-42fc-42d9-b926-d7fd8869e7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 47168us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=255 z=0 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMCHf_RD3Wd",
        "colab_type": "text"
      },
      "source": [
        "Change the experiment settings, by executing more iterations and compare the unidimensional kernel with the bidimensional kernel.\n",
        "\n",
        "Comment the results in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02hd2qlUEOsg",
        "colab_type": "code",
        "outputId": "cf8e97d6-1059-45ab-f2be-3c3c5a187d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name experiment_settings.h\n",
        "#pragma once\n",
        "#define WIDTH 3840\n",
        "#define HEIGHT 2160\n",
        "#define EXPERIMENT_ITERATIONS 100 //try different values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/experiment_settings.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3gBy530ektk",
        "colab_type": "text"
      },
      "source": [
        "## Section 3:\n",
        "\n",
        "Starting from Section 2, (use the best BLOCK_SIZE you found) try to optimize the memory accesses in some way, without using shared memory.\n",
        "\n",
        "Comment in the report which memory access problems you observe. Are the memory accesses aligned, and therfore coalesced?\n",
        "\n",
        "Remember that opposite to what the CPU compilers do, the nvcc compiler does not optimize the memory accesses in structs\n",
        "\n",
        "Remember also that GPU memory is organized in blocks of 4 bytes, and any array based on data elements that are not multiple of 2, will not be alligned. To be coalesced (specially in old architectures), it also has to be multiple of 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmP4PDZ-e4lG",
        "colab_type": "code",
        "outputId": "27f41ad3-da33-4d08-ee8a-d6800942ad75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL BETTER MEMORY ACCESS\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = threadIdx.x + (blockIdx.x * blockDim.x); //Use the same code as in section 2 in this line\n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\t\n",
        "\t    uchar3 tmp_3 = brg[x];\n",
        "      /*uint3 tmp_3 = ((uint3*)brg)[x];\n",
        "\n",
        "      uchar4* tmp1 = (uchar4*)(&tmp_3.x);\n",
        "      uchar4* tmp2 = (uchar4*)(&tmp_3.y);\n",
        "      uchar4* tmp3 = (uchar4*)(&tmp_3.z);\n",
        "\n",
        "      \n",
        "      uchar4 pix1 = make_uchar4(tmp1->y, tmp1->z, tmp1->x, 255);\n",
        "      uchar4 pix2 = make_uchar4(tmp2->x, tmp2->y, tmp1->w, 255);\n",
        "      uchar4 pix3 = make_uchar4(tmp2->w, tmp3->x, tmp2->z, 255);\n",
        "      uchar4 pix4 = make_uchar4(tmp3->z, tmp3->w, tmp3->y, 255);\n",
        "      ((uint4*)rgba)[x] = make_uint4(*(uint*)(&pix1), *(uint*)(&pix2), *(uint*)(&pix3), *(uint*)(&pix4));*/\n",
        "\n",
        "      uchar4 tmp_4;\n",
        "      \n",
        "      tmp_4.x = tmp_3.y;\n",
        "      tmp_4.y = tmp_3.z;\n",
        "      tmp_4.z = tmp_3.x;\n",
        "      tmp_4.w = 255;\n",
        "      \n",
        "      rgba[x] = tmp_4;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "  //dim3 grid(ceil(((width*height)/4)/(float)block.x), 1, 1);\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_brg, d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  FILE* fp = fopen(\"dades.txt\", \"a\");\n",
        "  fprintf(fp, \"%lf\\n\", (double) duration);\n",
        "  fclose(fp);\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMDGsQjPAkIv",
        "colab_type": "code",
        "outputId": "86b4c2ac-6b4f-4da1-ae51-86d8154fb6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name test3.cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/test3.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju7VOSl1_40V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc src/test3.cu -o test3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDLTNvktAGOK",
        "colab_type": "code",
        "outputId": "0bda52db-34f6-4944-b942-af4c913ef4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "!nvprof ./test3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==420== NVPROF is profiling process 420, command: ./test3\n",
            "convertBRG2RGBA time for 100 iterations = 38819us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=255 z=0 w=255\n",
            "Executed!! Results OK.\n",
            "==420== Profiling application: ./test3\n",
            "==420== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   58.61%  38.465ms       100  384.65us  384.13us  385.76us  convertBRG2RGBA(uchar3*, uchar4*, int, int)\n",
            "                   33.04%  21.681ms         1  21.681ms  21.681ms  21.681ms  [CUDA memcpy DtoH]\n",
            "                    8.35%  5.4820ms         1  5.4820ms  5.4820ms  5.4820ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.7600us         1  1.7600us  1.7600us  1.7600us  [CUDA memset]\n",
            "      API calls:   62.25%  208.70ms         2  104.35ms  253.04us  208.44ms  cudaMalloc\n",
            "                   17.09%  57.301ms         1  57.301ms  57.301ms  57.301ms  cudaDeviceReset\n",
            "                   11.37%  38.130ms         1  38.130ms  38.130ms  38.130ms  cudaDeviceSynchronize\n",
            "                    8.51%  28.542ms         2  14.271ms  5.6867ms  22.855ms  cudaMemcpy\n",
            "                    0.46%  1.5318ms         2  765.91us  185.95us  1.3459ms  cudaFree\n",
            "                    0.19%  643.17us       100  6.4310us  4.5830us  24.828us  cudaLaunchKernel\n",
            "                    0.06%  207.75us         1  207.75us  207.75us  207.75us  cuDeviceTotalMem\n",
            "                    0.04%  140.66us        97  1.4500us     134ns  65.562us  cuDeviceGetAttribute\n",
            "                    0.01%  48.697us         1  48.697us  48.697us  48.697us  cudaMemset\n",
            "                    0.01%  22.396us         1  22.396us  22.396us  22.396us  cuDeviceGetName\n",
            "                    0.00%  2.8270us         1  2.8270us  2.8270us  2.8270us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0780us         3     692ns     203ns  1.1700us  cuDeviceGetCount\n",
            "                    0.00%  1.5380us         2     769ns     335ns  1.2030us  cuDeviceGet\n",
            "                    0.00%     248ns         1     248ns     248ns     248ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPECp0T6IU0Z",
        "colab_type": "text"
      },
      "source": [
        "##Section 4:\n",
        "Now optimize the GPU memory accesses so that each thread always reads at least one element of 4 bytes. Use shared memory to that end.\n",
        "\n",
        "Look at the PDF Lab2CUDA in the campus, an read the last two pages. There you have a graphical explanation of the kernel issues. For this section you only need to understand the first figure.\n",
        "\n",
        "About shared memory: we will refresh some concepts.\n",
        "\n",
        "Shared memory, is a kind of memory that is visible only by the cuda threads of a thread block. Cuda threads from different thread blocks can not see the shared memory of other threadblocks.\n",
        "\n",
        "Shared memory is a limited resource. Depending on the GPU model, you may have from 32KB to 64KB of shared memory. Additionally, this memory is not used only by one threadblock. It is partitioned in as many independent blocks as thread blocks can execute in a single Streaming Multiprocessor (check the documentation if you don't know what a SM is). \n",
        "\n",
        "So when you are defining the amount of shared memory you want, you are defining the amount of memory, every thread block will have available.\n",
        "\n",
        "If you reserve 64KB of shared memory, in a GPU that has this capacity, only one thread block will execute on each SM, which is super slow. Each SM can concurrently execute from 8 to 32 thread blocks. For the best performance, you usually want the greatest amount of thread blocks active on each SM.\n",
        "\n",
        "Therefore, you what to use the least shared memory possible, and only use it when it has clear benefits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKRs22QeIvwj",
        "colab_type": "code",
        "outputId": "fdc25827-d8d0-4602-ac24-87460100ccae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different vaues of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int position = threadIdx.x + (blockIdx.x * N_ELEMS_3_4_TBLOCK);// use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK];\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) {\n",
        "      bgrShared[threadIdx.x] = reinterpret_cast<uchar4*>(brg)[position];\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "  \n",
        "  position = threadIdx.x + (blockIdx.x * blockDim.x); // recompute position without N_ELEMS_3_4_TBLOCK to write the results\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (position < width*height) {\t\n",
        "        uchar3 local = reinterpret_cast<uchar3*>(bgrShared)[threadIdx.x];\n",
        "        rgba[position] = make_uchar4(local.y,local.z,local.x,255);\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_brg, d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  FILE* fp = fopen(\"dades.txt\", \"a\");\n",
        "  fprintf(fp, \"%lf\\n\", (double) duration);\n",
        "  fclose(fp);\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E_ErYqLAtVU",
        "colab_type": "code",
        "outputId": "2c5c6638-a0f5-49cc-d04f-e0a24261920f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name section_4.cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/section_4.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GifNYIbXs1F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc src/section_4.cu -o section_4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-QNQsaitFcn",
        "colab_type": "code",
        "outputId": "8f8e5809-81aa-4e7a-b1df-6d11cdf3b4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!nvprof ./section_4 2> a.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 39266us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=255 z=0 w=255\n",
            "Executed!! Results OK.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIweqX0nu_v8",
        "colab_type": "code",
        "outputId": "f717871f-db6e-410e-f774-259bc08e06d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "!cat a.txt\n",
        "!cat dades.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==469== NVPROF is profiling process 469, command: ./section_4\n",
            "==469== Profiling application: ./section_4\n",
            "==469== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   59.32%  38.935ms       100  389.35us  388.09us  390.87us  convertBRG2RGBA(uchar3*, uchar4*, int, int)\n",
            "                   31.80%  20.872ms         1  20.872ms  20.872ms  20.872ms  [CUDA memcpy DtoH]\n",
            "                    8.87%  5.8240ms         1  5.8240ms  5.8240ms  5.8240ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.0240us         1  1.0240us  1.0240us  1.0240us  [CUDA memset]\n",
            "      API calls:   66.37%  242.99ms         2  121.49ms  290.32us  242.70ms  cudaMalloc\n",
            "                   14.71%  53.843ms         1  53.843ms  53.843ms  53.843ms  cudaDeviceReset\n",
            "                   10.53%  38.546ms         1  38.546ms  38.546ms  38.546ms  cudaDeviceSynchronize\n",
            "                    7.66%  28.054ms         2  14.027ms  6.0417ms  22.013ms  cudaMemcpy\n",
            "                    0.40%  1.4633ms         2  731.64us  168.41us  1.2949ms  cudaFree\n",
            "                    0.19%  677.39us       100  6.7730us  4.2130us  26.788us  cudaLaunchKernel\n",
            "                    0.07%  272.41us        97  2.8080us     133ns  189.98us  cuDeviceGetAttribute\n",
            "                    0.05%  182.79us         1  182.79us  182.79us  182.79us  cuDeviceTotalMem\n",
            "                    0.01%  44.886us         1  44.886us  44.886us  44.886us  cudaMemset\n",
            "                    0.01%  24.755us         1  24.755us  24.755us  24.755us  cuDeviceGetName\n",
            "                    0.00%  3.2190us         1  3.2190us  3.2190us  3.2190us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6730us         3     557ns     170ns  1.0640us  cuDeviceGetCount\n",
            "                    0.00%  1.0320us         2     516ns     224ns     808ns  cuDeviceGet\n",
            "                    0.00%     261ns         1     261ns     261ns     261ns  cuDeviceGetUuid\n",
            "52725.000000\n",
            "47168.000000\n",
            "38819.000000\n",
            "39266.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGPcLMKz6a0d",
        "colab_type": "text"
      },
      "source": [
        "##Section 5:\n",
        "\n",
        "Now, following the explanation in the Lab2CUDA, try to implement the described algorithm. Take into account that the piece of code that reads from temp variables and writes in pix_write, requires some changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DvT4yBO6ck2",
        "colab_type": "code",
        "outputId": "81d7f099-e33d-4d33-d33c-06af0701eeec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different vaues of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "#define N_ELEMS_3_4_TBLOCK_3 N_ELEMS_3_4_TBLOCK/3\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int position = threadIdx.x + (blockIdx.x * N_ELEMS_3_4_TBLOCK);// use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK];\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) {\n",
        "      bgrShared[threadIdx.x] = reinterpret_cast<uchar4*>(brg)[position];\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  /* pix_write -> La memoria para acceso  */\n",
        "  __shared__ uchar4 pix_write[1024];\n",
        "\n",
        "  /* Cada 3 threads escribimos los bytes a memoria compartida */\n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK_3)\n",
        "  {\n",
        "      /* Calculem els temp */\n",
        "      uchar4 temp1, temp2, temp3;\n",
        "      temp1 = bgrShared[3 * threadIdx.x];\n",
        "      temp2 = bgrShared[3 * threadIdx.x + 1];\n",
        "      temp3 = bgrShared[3 * threadIdx.x + 2];\n",
        "\n",
        "      int position2 = threadIdx.x + (blockIdx.x * N_ELEMS_3_4_TBLOCK_3);\n",
        "\n",
        "      /* Asignamos la memoria */\n",
        "      pix_write[threadIdx.x * 4]     = make_uchar4(temp1.y, temp1.z, temp1.x, 255);\n",
        "      pix_write[threadIdx.x * 4 + 1] = make_uchar4(temp2.x, temp2.y, temp1.w, 255);\n",
        "      pix_write[threadIdx.x * 4 + 2] = make_uchar4(temp2.w, temp3.x, temp2.z, 255);\n",
        "      pix_write[threadIdx.x * 4 + 3] = make_uchar4(temp3.z, temp3.w, temp3.y, 255);\n",
        "   \n",
        "      ((uint4*)rgba)[position2] = ((uint4*)pix_write)[threadIdx.x];\n",
        "  }\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_brg, d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  FILE* fp = fopen(\"dades.txt\", \"a\");\n",
        "  fprintf(fp, \"%lf\\n\", (double) duration);\n",
        "  fclose(fp);\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-p5Lxq7II5t",
        "colab_type": "code",
        "outputId": "67017b45-a5bd-4bb9-8498-b28956572aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name section_5.cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/section_5.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5O0rdnHsNqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc src/section_5.cu -o section_5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep38RfMcsiEZ",
        "colab_type": "code",
        "outputId": "1a87a914-dde6-4548-9e46-bc34c6278b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "!nvprof ./section_5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==522== NVPROF is profiling process 522, command: ./section_5\n",
            "convertBRG2RGBA time for 100 iterations = 40322us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=255 z=0 w=255\n",
            "Executed!! Results OK.\n",
            "==522== Profiling application: ./section_5\n",
            "==522== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   60.39%  39.987ms       100  399.87us  397.46us  401.81us  convertBRG2RGBA(uchar3*, uchar4*, int, int)\n",
            "                   31.29%  20.720ms         1  20.720ms  20.720ms  20.720ms  [CUDA memcpy DtoH]\n",
            "                    8.32%  5.5075ms         1  5.5075ms  5.5075ms  5.5075ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.0240us         1  1.0240us  1.0240us  1.0240us  [CUDA memset]\n",
            "      API calls:   62.46%  206.99ms         2  103.49ms  257.22us  206.73ms  cudaMalloc\n",
            "                   16.46%  54.564ms         1  54.564ms  54.564ms  54.564ms  cudaDeviceReset\n",
            "                   11.98%  39.714ms         1  39.714ms  39.714ms  39.714ms  cudaDeviceSynchronize\n",
            "                    8.28%  27.437ms         2  13.719ms  5.6787ms  21.758ms  cudaMemcpy\n",
            "                    0.46%  1.5236ms         2  761.82us  205.78us  1.3179ms  cudaFree\n",
            "                    0.17%  573.58us       100  5.7350us  4.4780us  17.075us  cudaLaunchKernel\n",
            "                    0.10%  335.52us        97  3.4590us     138ns  233.44us  cuDeviceGetAttribute\n",
            "                    0.06%  197.61us         1  197.61us  197.61us  197.61us  cuDeviceTotalMem\n",
            "                    0.01%  44.442us         1  44.442us  44.442us  44.442us  cudaMemset\n",
            "                    0.01%  27.210us         1  27.210us  27.210us  27.210us  cuDeviceGetName\n",
            "                    0.00%  2.9230us         1  2.9230us  2.9230us  2.9230us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7180us         3     572ns     176ns  1.1710us  cuDeviceGetCount\n",
            "                    0.00%  1.1730us         2     586ns     265ns     908ns  cuDeviceGet\n",
            "                    0.00%     308ns         1     308ns     308ns     308ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MazFSZvFO9lT",
        "colab_type": "text"
      },
      "source": [
        "##Section 6:\n",
        "\n",
        "Change all the host code necessary, to use cuda streams. Here you have an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EvgoMT5PVYn",
        "colab_type": "code",
        "outputId": "9e757796-6941-4e07-ad57-0d8527f24dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void square(int* d_input, int* d_output) {\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    int val = d_input[x];\n",
        "    // We exploit the temporal locality of the value stored in d_output[x]\n",
        "    d_output[x] = val*val;\n",
        "}\n",
        "\n",
        "static const size_t dataSize = sizeof(int)*1024;\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    int *h_input, *h_output;\n",
        "    h_input = (int*)malloc(dataSize);\n",
        "    h_output = (int*)malloc(dataSize);\n",
        "\n",
        "    for (int i=0; i<1024; ++i) h_input[i]=i;\n",
        "\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, dataSize);\n",
        "    cudaMalloc(&d_output, dataSize);\n",
        "\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    dim3 block(512);\n",
        "    dim3 grid(2);\n",
        "\n",
        "    // The CPU thread does not wait that any of the following actions finish\n",
        "    // It only asks the GPU to do the copies and the kernel and continues\n",
        "    cudaMemcpyAsync(d_input, h_input, dataSize, cudaMemcpyHostToDevice, stream);\n",
        "    square<<<grid, block, 0, stream>>>(d_input, d_output);\n",
        "    cudaMemcpyAsync(h_output, d_output, dataSize, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    // Here, we wait for all the orders enqueued in stream, to finish.\n",
        "    cudaStreamSynchronize(stream);\n",
        "\n",
        "    bool correct = true;\n",
        "    for (int i=0; i<1024; ++i) correct &= h_output[i] == i*i;\n",
        "\n",
        "    std::cout << \"Finished and results are \" << (correct ? \"correct.\" : \"not correct.\") << std::endl;\n",
        "\n",
        "    cudaStreamDestroy(stream);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished and results are correct.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4VK6ylPWaEY",
        "colab_type": "text"
      },
      "source": [
        "Modify this code, to use streams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz3kVFkrWOlB",
        "colab_type": "code",
        "outputId": "33f9d430-ca53-41e8-9de3-ddded2e38920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cudaSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaMalloc(d_rgba, sizeof(uchar4)*width*height), \"Alloc d_rgba:\");\n",
        "}\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice, stream), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemsetAsync(d_rgba, 0, width*height*sizeof(uchar4), stream), \"Memset d_rgba:\");\n",
        "}\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaFree(d_brg), \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaFree(d_rgba), \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/memory_functions.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymPFWGqkWew-",
        "colab_type": "text"
      },
      "source": [
        "And also modify this code, so that mem copies from CPU to GPU and from GPU to CPU use an stream, and are not blocking.\n",
        "\n",
        "Additionally, add a chrono between the first memcpy (included) and the cudaStreamSynchronize. This is the time you will have to compare.\n",
        "\n",
        "Follow the indications in the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih1ADZpjWeFk",
        "colab_type": "code",
        "outputId": "6a758f28-3aaf-4003-cdb4-3c59cc16bfac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = HEIGHT/3;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  cudaStream_t stream;\n",
        "  cudaStreamCreate(&stream);\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  \n",
        "  // Start measuring time here\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba, stream);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS, stream);\n",
        "\n",
        "  // Copy data back from GPU to CPU\n",
        "  CU_CHECK2(cudaMemcpyAsync(h_rgba, d_rgba, sizeof(uchar4)*WIDTH*HEIGHT, cudaMemcpyDeviceToHost, stream), \"Cuda memcpy Device to Host: \");\n",
        "\n",
        "  // Synchronize the stream here\n",
        "  cudaStreamSynchronize(stream);\n",
        "  // Stop measuring time here, and print it\n",
        "  \n",
        "    \n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/experiment.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cb3WvwUW6E7",
        "colab_type": "text"
      },
      "source": [
        "Do the following:\n",
        "\n",
        "1.   Use the fastest kernel version.\n",
        "2.   Use number of iterations = 1.\n",
        "3.   Compare the same kernel, with the original Host code, and this new Host code.\n",
        "4.   To do so, you can use the code you do now, you only need to set stream=0 in order to simulate the original code.\n",
        "5.   Execute it with the following code.\n",
        "6.   Compare and try to explain the performance difference in the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icJq5yuCW9f_",
        "colab_type": "code",
        "outputId": "06e739d0-eb76-4541-80b3-64536e99265b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 39031us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=0 z=255 w=255\n",
            "First position x=0 y=255 z=0 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FIPUf9Z-5lB",
        "colab_type": "code",
        "outputId": "9ea3e038-f5d1-4058-9f0e-ce8779cd14cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!cat dades.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52725.000000\n",
            "47168.000000\n",
            "38819.000000\n",
            "39266.000000\n",
            "40322.000000\n",
            "39031.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLyRkpQsAWbl",
        "colab_type": "code",
        "outputId": "fccfbca2-2a94-4191-dc30-0f205b115a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "\"\"\"\n",
        "Script en python per a visualitzar una comparativa entre els resultats.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_array_from_file(filename: str) -> list:\n",
        "  with open(filename, \"r\") as f:\n",
        "    arr = [int(line.split(\".\")[0]) for line in f]\n",
        "  return arr\n",
        "\n",
        "\n",
        "objects = ('Section 1', 'Section 2', 'Section 3', 'Section 4', 'Section 5', 'Section 6')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = get_array_from_file(\"dades.txt\")\n",
        "\n",
        "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.ylabel('Temps per solucionar el problema')\n",
        "plt.title('Rapidesa per secció en us (per 100 iteracions)')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7xEU6iRpkE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}