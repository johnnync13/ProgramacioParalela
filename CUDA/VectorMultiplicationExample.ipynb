{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VectorMultiplicationExample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii_yEQdxULw9",
        "colab_type": "text"
      },
      "source": [
        "# Vector multiplication example\n",
        "This is an example developed live in class.\n",
        "\n",
        "We will use it in two different ways so you can see a nice way to measure the execution times of each part of the code.\n",
        "\n",
        "First example is written to be directly executed and to play with it.\n",
        "\n",
        "The second one is the same, but instead of using %%cu, we use %cuda and save the code into a file. We also removed the shell output so that you can try with bigger examples. Then, by compiling it manually, we will be able to use nvprof, to measure each part execution time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4wGwiuAU_mg",
        "colab_type": "text"
      },
      "source": [
        "##First implementation to see results and understand\n",
        "\n",
        "First we install the pluggin and check which GPU we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aTX86A-OsAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "52cda094-d240-4e21-ad1c-4e70f4426823"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-a4i97kjz\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-a4i97kjz\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=454422906add62d9318c38584a90889b0ad095ac0b2156fc1d0063122b86731b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wa3hvj4f/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVUikdO0Oupf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main() {\n",
        "    int numDevs=0;\n",
        "    cudaGetDeviceCount(&numDevs);\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, 0);\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVbOV3orVwXf",
        "colab_type": "text"
      },
      "source": [
        "Now the example. It is a vector point to point multiplication, of any size.\n",
        "\n",
        "We do all the steps in order to have the data in the GPU to perform the computation, and back to the CPU to read the results.\n",
        "\n",
        "Since we print de results, be carefull not to use a too big vector size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xpZkwirWS5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <iostream>\n",
        "#include \"math.h\"\n",
        "\n",
        "#define VEC_SIZE 8\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void vectMult(const int* d_vectA, const int* d_vectB, int* d_vectC) {\n",
        "    int x = threadIdx.x + (blockIdx.x * blockDim.x);\n",
        "    if (x < VEC_SIZE) {\n",
        "      d_vectC[x] = d_vectA[x] * d_vectB[x];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int *h_vectA, *h_vectB, *d_vectA, *d_vectB, *h_vectC, *d_vectC;\n",
        "\n",
        "    h_vectA = (int*)malloc(sizeof(int)*VEC_SIZE);\n",
        "    h_vectB = (int*)malloc(sizeof(int)*VEC_SIZE);\n",
        "    h_vectC = (int*)malloc(sizeof(int)*VEC_SIZE);\n",
        "\n",
        "    cudaMalloc(&d_vectA, sizeof(int)*VEC_SIZE);\n",
        "    cudaMalloc(&d_vectB, sizeof(int)*VEC_SIZE);\n",
        "    cudaMalloc(&d_vectC, sizeof(int)*VEC_SIZE);\n",
        "\n",
        "    for (int i=0; i<VEC_SIZE; ++i) {\n",
        "        h_vectA[i] = i;\n",
        "        h_vectB[i] = i;\n",
        "    }\n",
        "\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    cudaMemcpyAsync(d_vectA, h_vectA, sizeof(int)*VEC_SIZE, cudaMemcpyHostToDevice, stream);\n",
        "    cudaMemcpyAsync(d_vectB, h_vectB, sizeof(int)*VEC_SIZE, cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    dim3 block;\n",
        "    dim3 grid;\n",
        "\n",
        "    if (VEC_SIZE <= BLOCK_SIZE) {\n",
        "        block.x = VEC_SIZE;\n",
        "        grid.x = 1;\n",
        "    } else {\n",
        "        block.x = BLOCK_SIZE;\n",
        "        grid.x = ceil((double)VEC_SIZE / (double)BLOCK_SIZE);\n",
        "    }\n",
        "\n",
        "    vectMult<<<grid, block, 0, stream>>>(d_vectA, d_vectB, d_vectC);\n",
        "\n",
        "    cudaMemcpyAsync(h_vectC, d_vectC, sizeof(int)*VEC_SIZE, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    cudaStreamSynchronize(stream);\n",
        "\n",
        "    std::cout << \"Results: \";\n",
        "    for (int i=0; i<VEC_SIZE; ++i) {\n",
        "        std::cout << h_vectC[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6najhIgWV6t",
        "colab_type": "text"
      },
      "source": [
        "##Second implementation with manual compilation and profiling\n",
        "Now we do the same, but we save the code into test.cu.\n",
        "\n",
        "This way we can compile it manually with nvcc, and then use nvprof to see detailed execution times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF8krrnMOiVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%cuda --name test.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include \"math.h\"\n",
        "\n",
        "#define VEC_SIZE 4000\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void vectMult(const int* d_vectA, const int* d_vectB, int* d_vectC) {\n",
        "    int x = threadIdx.x + (blockIdx.x * blockDim.x);\n",
        "    if (x < VEC_SIZE) {\n",
        "      d_vectC[x] = d_vectA[x] * d_vectB[x];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int *h_vectA, *h_vectB, *d_vectA, *d_vectB, *h_vectC, *d_vectC;\n",
        "\n",
        "    h_vectA = (int*)malloc(sizeof(int)*VEC_SIZE);\n",
        "    h_vectB = (int*)malloc(sizeof(int)*VEC_SIZE);\n",
        "    h_vectC = (int*)malloc(sizeof(int)*VEC_SIZE);\n",
        "\n",
        "    cudaMalloc(&d_vectA, sizeof(int)*VEC_SIZE);\n",
        "    cudaMalloc(&d_vectB, sizeof(int)*VEC_SIZE);\n",
        "    cudaMalloc(&d_vectC, sizeof(int)*VEC_SIZE);\n",
        "\n",
        "    for (int i=0; i<VEC_SIZE; ++i) {\n",
        "        h_vectA[i] = i;\n",
        "        h_vectB[i] = i;\n",
        "    }\n",
        "\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    cudaMemcpyAsync(d_vectA, h_vectA, sizeof(int)*VEC_SIZE, cudaMemcpyHostToDevice, stream);\n",
        "    cudaMemcpyAsync(d_vectB, h_vectB, sizeof(int)*VEC_SIZE, cudaMemcpyHostToDevice, stream);\n",
        "\n",
        "    dim3 block;\n",
        "    dim3 grid;\n",
        "\n",
        "    if (VEC_SIZE <= BLOCK_SIZE) {\n",
        "        block.x = VEC_SIZE;\n",
        "        grid.x = 1;\n",
        "    } else {\n",
        "        block.x = BLOCK_SIZE;\n",
        "        grid.x = ceil((double)VEC_SIZE / (double)BLOCK_SIZE);\n",
        "    }\n",
        "\n",
        "    vectMult<<<grid, block, 0, stream>>>(d_vectA, d_vectB, d_vectC);\n",
        "\n",
        "    cudaMemcpyAsync(h_vectC, d_vectC, sizeof(int)*VEC_SIZE, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    cudaStreamSynchronize(stream);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kblvZJB6SuBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc src/test.cu -o test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86pZPoxCSxFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvprof ./test"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}